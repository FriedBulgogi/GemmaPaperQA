{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Libraries and Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: transformers in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dotenv in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyPDF2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: langchain in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (0.1.130)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install transformers\n",
    "!pip install python-dotenv\n",
    "!pip install PyPDF2\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Disable WANDB to avoid logging issues during training or inference\n",
    "os.environ['WANDB_DISABLED'] = \"true\"\n",
    "\n",
    "# Constants for file paths and model\n",
    "PDF_DIRECTORY = \"/path/to/directory/containing/papers/\"     # Directory path containing PDF papers\n",
    "PDF_FILENAME = \"your_paper_name.pdf\"                        # Name of the PDF file\n",
    "MODEL_NAME = \"/path/to/your/model/\"                         # Path to the model directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Load and extract text from a PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    if os.path.exists(pdf_path):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            pdf_reader = PdfReader(f)\n",
    "            text = \"\".join(page.extract_text() for page in pdf_reader.pages)\n",
    "        return text\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error: PDF file '{pdf_path}' not found.\")\n",
    "\n",
    "\n",
    "def split_text(text):\n",
    "    \"\"\"\n",
    "    Split the extracted text into chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text extracted from the PDF.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",             \n",
    "        chunk_size=1000,            \n",
    "        chunk_overlap=200,          \n",
    "        length_function=len         \n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "def create_knowledge_base(chunks):\n",
    "    \"\"\"\n",
    "    Create a FAISS knowledge base from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): A list of text chunks.\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: A FAISS knowledge base object.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    return FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "    Load the HuggingFace model and tokenizer, and create a text-generation pipeline.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A HuggingFace pipeline for text generation.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150, temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Main Execution Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" Main function to run the PDF question-answering system. \"\"\"\n",
    "\n",
    "    pdf_path = os.path.join(PDF_DIRECTORY, PDF_FILENAME)\n",
    "    \n",
    "    # Load and process the PDF\n",
    "    try:\n",
    "        text = load_pdf(pdf_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    chunks = split_text(text)\n",
    "    knowledge_base = create_knowledge_base(chunks)\n",
    "\n",
    "    # Load the language model\n",
    "    try:\n",
    "        pipe = load_model(MODEL_NAME)  # Use the updated model name\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "    # Interaction loop for user questions\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question about the PDF (or type 'exit' to quit): \").strip()\n",
    "\n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Exiting the program.\")   # Exit message\n",
    "            break\n",
    "\n",
    "        if user_question:\n",
    "            # Perform similarity search and question-answering\n",
    "            docs = knowledge_base.similarity_search(user_question)  \n",
    "            response = chain.run(input_documents=docs, question=user_question)\n",
    "\n",
    "            if \"Helpful Answer:\" in response:\n",
    "                response = response.split(\"Helpful Answer:\")[1].strip()\n",
    "\n",
    "            print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1755089/2817442548.py:49: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/tmp/ipykernel_1755089/2817442548.py:49: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.89s/it]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/tmp/ipykernel_1755089/4026149746.py:24: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/tmp/ipykernel_1755089/4026149746.py:25: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/v0.2/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
      "/tmp/ipykernel_1755089/4026149746.py:38: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(input_documents=docs, question=user_question)\n",
      "/home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Attention is a function that maps a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/miniconda3/envs/halyn/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
